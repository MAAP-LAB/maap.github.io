"""
Google Colabìš© BLAP LoRA í›ˆë ¨ ìŠ¤í¬ë¦½íŠ¸
Colab ë…¸íŠ¸ë¶ì—ì„œ ì§ì ‘ ì‹¤í–‰ ê°€ëŠ¥
"""

import subprocess
import sys
from pathlib import Path

def install_requirements():
    """í•„ìš”í•œ íŒ¨í‚¤ì§€ë“¤ ì„¤ì¹˜"""
    print("ğŸ“¦ Installing required packages...")
    
    packages = [
        "peft",
        "transformers==4.30.0",
        "torch",
        "matplotlib",
        "numpy",
        "tqdm"
    ]
    
    for package in packages:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    
    print("âœ… All packages installed!")

def run_lora_training():
    """LoRA í›ˆë ¨ ì‹¤í–‰"""
    print("ğŸš€ Starting BLAP LoRA Fine-tuning in Colab")
    print("=" * 50)
    
    # Colab í™˜ê²½ì—ì„œì˜ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    base_dir = "/content"  # Colabì˜ ê¸°ë³¸ ì‘ì—… ë””ë ‰í† ë¦¬
    
    # íŒŒì¼ ê²½ë¡œë“¤ (Colabì—ì„œ ì—…ë¡œë“œí•œ ìœ„ì¹˜ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
    config = {
        "blap_checkpoint": f"{base_dir}/blap/checkpoint/checkpoint.ckpt",
        "clamp3_weights": f"{base_dir}/clamp3/code/weights.pth", 
        "config_path": f"{base_dir}/blap/checkpoint/config.json",
        "train_json": f"{base_dir}/Adapters/FinetuneMusicQA_npy.json",
        "val_json": f"{base_dir}/Adapters/EvalMusicQA_npy.json",
        "save_dir": f"{base_dir}/lora_checkpoints",
        
        # LoRA í•˜ì´í¼íŒŒë¼ë¯¸í„°
        "lora_r": 16,
        "lora_alpha": 32, 
        "lora_dropout": 0.1,
        
        # í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        "batch_size": 4,  # Colab GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì‘ê²Œ ì„¤ì •
        "epochs": 12,
        "lr": 2e-4,
        "accumulation_steps": 8,  # ì‘ì€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ë³´ìƒ
    }
    
    # ëª…ë ¹ì–´ êµ¬ì„±
    cmd = [
        sys.executable, f"{base_dir}/Adapters/blap_lora_trainer.py",
        "--blap_checkpoint", config["blap_checkpoint"],
        "--clamp3_weights", config["clamp3_weights"],
        "--config_path", config["config_path"],
        "--train_json", config["train_json"],
        "--val_json", config["val_json"],
        "--batch_size", str(config["batch_size"]),
        "--epochs", str(config["epochs"]),
        "--lr", str(config["lr"]),
        "--accumulation_steps", str(config["accumulation_steps"]),
        "--lora_r", str(config["lora_r"]),
        "--lora_alpha", str(config["lora_alpha"]),
        "--lora_dropout", str(config["lora_dropout"]),
        "--save_dir", config["save_dir"]
    ]
    
    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    
    # í›ˆë ¨ ì‹¤í–‰
    try:
        subprocess.run(cmd, check=True)
        print("ğŸ‰ LoRA Fine-tuning completed successfully!")
        print(f"Check results in: {config['save_dir']}")
        
    except subprocess.CalledProcessError as e:
        print(f"âŒ Training failed with error: {e}")
        print("Please check the file paths and GPU availability.")

if __name__ == "__main__":
    # íŒ¨í‚¤ì§€ ì„¤ì¹˜
    install_requirements()
    
    # LoRA í›ˆë ¨ ì‹¤í–‰
    run_lora_training()